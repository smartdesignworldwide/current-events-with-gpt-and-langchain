{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c5dae9-9fc7-4245-858e-bd81e026869f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring the Nuances of Customizing GPT by Teaching GPT about Current Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c8ff8-6e31-4e20-b306-98a81b7ec051",
   "metadata": {},
   "source": [
    "With all the hype around ChatGPT, we wanted to explore what it's like to develop using OpenAI's APIs. What would it be like to train a model using GPT-3 as a platform? How much training data would it take? How much would it cost? What would we learn and how hard would it be?\n",
    "\n",
    "In order to learn more, we needed to pick a niche that GPT-3 didn't know about. We chose current events. The latest version of GPT-3 (often referred to as GPT-3.5) is trained on data thru June 2021. Anything after this, the model doesn't know about. By downloading RSS feeds of popular news sites, we could teach GPT-3 about things it didn't know. We could then test it out by asking the new model questions about events that happened after June 2021.\n",
    "\n",
    "OpenAI offers a service called fine tuning which allows you to customize a model by feeding it prompts and responses which would be examples of what it should learn. This was our first approach to solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac641b-37d1-4113-96f5-94a99ed15425",
   "metadata": {},
   "source": [
    "## Fine Tuning with OpenAI's GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a87bf-9374-4e23-b1e4-2a7a63dcc748",
   "metadata": {},
   "source": [
    "The first thing we did was install OpenAI's Python package, then chose to train it on a topic that required recent information: the train derailment in Ohio in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f08264-dcb3-4fc7-accb-d0a1fa788def",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/carter/.local/lib/python3.10/site-packages (0.26.3)\n",
      "Collecting openai\n",
      "  Using cached openai-0.26.5-py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.20 in /usr/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /home/carter/.local/lib/python3.10/site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/carter/.local/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/carter/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.26.3\n",
      "    Uninstalling openai-0.26.3:\n",
      "      Successfully uninstalled openai-0.26.3\n",
      "Successfully installed openai-0.26.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a063fb9-0b10-44a6-a71d-5aaa1ed4a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2fd21-a6f8-4e09-bb85-eb6d74707e70",
   "metadata": {},
   "source": [
    "We'll also need to setup our OpenAI key that we obtained from OpenAI.com. OpenAI gives you $18 in free credit which is more than enough to run this notebook and do much more experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb448c2-87be-4909-a0e2-a776eb5e203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"Add OpenAI key here\"\n",
    "openai.api_key = \"Add OpenAI key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fe98f-dcc1-4d5d-a0e1-ea4b5ea6c076",
   "metadata": {},
   "source": [
    "As a baseline let's query OpenAI's state of the art GPT-3.5 model, Davinci and ask it about the recent train derailment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a84d8d35-9299-42e1-937c-9bb9518282c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where did the train carrying hazardous materials derail?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b621b97-c794-4317-92f0-9e2162580dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The exact location of the train derailment is not available, as different\n"
     ]
    }
   ],
   "source": [
    "result = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt\n",
    ")\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a1128-5d8a-414b-be38-2e47bf3738da",
   "metadata": {},
   "source": [
    "Well, that didn't work. Let's try fine tuning a model by adding some data to the model about when the train derailment in Ohio. We'll prepare the data, save it to a file and then upload that file to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c5a8006-8e55-46fd-a5a3-6ff9dc48b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://en.wikipedia.org/wiki/2023_Ohio_train_derailment\n",
    "examples = [\n",
    "    {\"prompt\": \"2023 Ohio train derailment\", \"completion\": \"The 2023 Ohio train derailment (also called the East Palestine train derailment) occurred on February 3, 2023, at 8:55 p.m. EST (UTCâˆ’5), when a Norfolk Southern freight train carrying hazardous materials derailed in East Palestine, Ohio, United States.[1] The freight train burned for more than two days, and then emergency crews conducted a controlled burn of several railcars at the request of state officials,[2] which released hydrogen chloride and phosgene into the air.[1] As a result, residents within a 1-mile (1.6-kilometer) radius were evacuated, and an emergency response was initiated from agencies in Ohio, Pennsylvania, and West Virginia. The U.S. federal government sent Environmental Protection Agency (EPA) administrator Michael S. Regan to provide assistance on February 16, 2023.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf732a9d-19fa-4be6-bc9d-1cf490a9e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"trainingdata.jsonl\", \"w\")\n",
    "\n",
    "for example in examples:\n",
    "    f.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c2e027e-aed8-46e0-932b-f179db3679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = openai.File.create(file=open(\"trainingdata.jsonl\"), purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212aebfa-d0ff-4cd8-8a4f-6f588c0006ef",
   "metadata": {},
   "source": [
    "From here we can tell OpenAI to begin fine tuning a model using Davinci as a base model but we'll add the additional information about the 2023 train derailment in Ohio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bda137e-02f8-4f1d-87df-c7b0b2362518",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune = openai.FineTune.create(training_file=file['id'], model=\"davinci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ae67-8816-48e4-befd-78f9b68c74b1",
   "metadata": {},
   "source": [
    "We can use the following console command to track the fine tuning's progress. It'll likely take about 30 minutes for this to be complete and query against the new model. If the command fails you can run it again to continue polling the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1b480cf-7524-4166-8c8a-3cd0c716f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-23 15:59:43] Created fine-tune: ft-av2Lfjr4eAObz9DFPPS7WX6G\n",
      "[2023-02-23 16:06:37] Fine-tune costs $0.02\n",
      "[2023-02-23 16:06:37] Fine-tune enqueued\n",
      "[2023-02-23 18:16:05] Fine-tune is in the queue. Queue number: 31\n",
      "[2023-02-23 18:17:02] Fine-tune is in the queue. Queue number: 30\n",
      "[2023-02-23 18:17:30] Fine-tune is in the queue. Queue number: 29\n",
      "[2023-02-23 18:19:17] Fine-tune is in the queue. Queue number: 28\n",
      "[2023-02-23 18:19:50] Fine-tune is in the queue. Queue number: 27\n",
      "[2023-02-23 18:20:19] Fine-tune is in the queue. Queue number: 26\n",
      "[2023-02-23 18:20:21] Fine-tune is in the queue. Queue number: 25\n",
      "[2023-02-23 18:25:59] Fine-tune is in the queue. Queue number: 24\n",
      "[2023-02-23 18:26:23] Fine-tune is in the queue. Queue number: 23\n",
      "[2023-02-23 18:29:01] Fine-tune is in the queue. Queue number: 22\n",
      "[2023-02-23 18:29:37] Fine-tune is in the queue. Queue number: 21\n",
      "[2023-02-23 18:30:57] Fine-tune is in the queue. Queue number: 20\n",
      "[2023-02-23 18:31:23] Fine-tune is in the queue. Queue number: 19\n",
      "[2023-02-23 18:31:32] Fine-tune is in the queue. Queue number: 18\n",
      "[2023-02-23 18:31:39] Fine-tune is in the queue. Queue number: 17\n",
      "[2023-02-23 18:31:55] Fine-tune is in the queue. Queue number: 16\n",
      "[2023-02-23 18:33:16] Fine-tune is in the queue. Queue number: 15\n",
      "[2023-02-23 18:34:54] Fine-tune is in the queue. Queue number: 14\n",
      "[2023-02-23 18:35:18] Fine-tune is in the queue. Queue number: 13\n",
      "[2023-02-23 18:35:20] Fine-tune is in the queue. Queue number: 12\n",
      "[2023-02-23 18:35:43] Fine-tune is in the queue. Queue number: 11\n",
      "[2023-02-23 18:35:54] Fine-tune is in the queue. Queue number: 10\n",
      "[2023-02-23 18:36:10] Fine-tune is in the queue. Queue number: 9\n",
      "[2023-02-23 18:38:32] Fine-tune is in the queue. Queue number: 8\n",
      "[2023-02-23 18:39:04] Fine-tune is in the queue. Queue number: 7\n",
      "[2023-02-23 18:40:50] Fine-tune is in the queue. Queue number: 6\n",
      "[2023-02-23 18:41:17] Fine-tune is in the queue. Queue number: 5\n",
      "[2023-02-23 18:41:41] Fine-tune is in the queue. Queue number: 4\n",
      "[2023-02-23 18:44:30] Fine-tune is in the queue. Queue number: 3\n",
      "[2023-02-23 18:44:36] Fine-tune is in the queue. Queue number: 2\n",
      "[2023-02-23 18:50:15] Fine-tune is in the queue. Queue number: 1\n",
      "[2023-02-23 18:51:39] Fine-tune is in the queue. Queue number: 0\n",
      "[2023-02-23 18:54:56] Fine-tune started\n",
      "[2023-02-23 18:56:36] Completed epoch 1/4\n",
      "[2023-02-23 18:56:36] Completed epoch 2/4\n",
      "[2023-02-23 18:56:37] Completed epoch 3/4\n",
      "[2023-02-23 18:56:37] Completed epoch 4/4\n",
      "[2023-02-23 18:57:20] Uploaded model: davinci:ft-personal-2023-02-24-01-57-19\n",
      "[2023-02-23 18:57:21] Uploaded result file: file-8c66wdOQu1FflBU6IPfGCFbF\n",
      "[2023-02-23 18:57:21] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m davinci:ft-personal-2023-02-24-01-57-19 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i {fine_tune['id']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6224bba-693e-40a0-895a-dc1eff026bcb",
   "metadata": {},
   "source": [
    "Once this is complete, let's copy the model below and try running our previous prompt to see if it does any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "696fa770-3634-406c-a4cd-309fc6bce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Officials say the train derailed in Nantes Dorian, just west of\n"
     ]
    }
   ],
   "source": [
    "result = openai.Completion.create(\n",
    "    model=\"davinci:ft-personal-2023-02-24-01-57-19\",\n",
    "    prompt=prompt\n",
    ")\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6527c15-01d8-4a48-b340-eb83a3dd21f4",
   "metadata": {},
   "source": [
    "We really don't see an improvement here. Perhaps it's because we need more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b44232-f646-47e4-b8a1-abc56234dec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine Tuning from RSS feeds on more data\n",
    "Let's start building out a more complex example that downloads all of today's news via RSS feeds and fine tunes based on that. First we'll install an RSS parser, have it download several popular news sources, and prepare our data to fine tune a new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c4077d-49ba-41a4-856a-1bfa07a797ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rss-parser in /home/carter/.local/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /usr/lib/python3.10/site-packages (from rss-parser) (4.9.2)\n",
      "Requirement already satisfied: requests>=2.24.0 in /usr/lib/python3.10/site-packages (from rss-parser) (2.28.1)\n",
      "Requirement already satisfied: bs4>=0.0.1 in /home/carter/.local/lib/python3.10/site-packages (from rss-parser) (0.0.1)\n",
      "Requirement already satisfied: pytest<8.0.0,>=7.1.2 in /usr/lib/python3.10/site-packages (from rss-parser) (7.2.1)\n",
      "Requirement already satisfied: pydantic>=1.6.1 in /usr/lib/python3.10/site-packages (from rss-parser) (1.10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.10/site-packages (from bs4>=0.0.1->rss-parser) (4.11.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/lib/python3.10/site-packages (from pydantic>=1.6.1->rss-parser) (4.4.0)\n",
      "Requirement already satisfied: iniconfig in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (22.2.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (21.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (1.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/lib/python3.10/site-packages (from pytest<8.0.0,>=7.1.2->rss-parser) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.24.0->rss-parser) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.24.0->rss-parser) (1.26.12)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.10/site-packages (from beautifulsoup4->bs4>=0.0.1->rss-parser) (2.3.2.post1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging->pytest<8.0.0,>=7.1.2->rss-parser) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install rss-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "345e7b8b-6855-4e95-89fe-a651cf36c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rss_parser import Parser\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae3793e8-e38d-46f2-8bc9-141421edaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_urls = [\n",
    "    \"https://rss.nytimes.com/services/xml/rss/nyt/US.xml\",\n",
    "    \"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"http://feeds.bbci.co.uk/news/rss.xml?edition=us\",\n",
    "    \"http://rss.cnn.com/rss/cnn_world.rss\",\n",
    "    \"http://rss.cnn.com/rss/cnn_us.rss\",\n",
    "    \"https://feeds.washingtonpost.com/rss/world?itid=lk_inline_manual_36\",\n",
    "    \"https://feeds.washingtonpost.com/rss/national?itid=lk_inline_manual_32\",\n",
    "    \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\",\n",
    "    \"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n",
    "    \"https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75b3217c-38f4-40ff-9655-4d47cbf284e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5391c657-1e98-4479-80a2-ffe4389c857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://rss.nytimes.com/services/xml/rss/nyt/US.xml\n",
      "https://rss.nytimes.com/services/xml/rss/nyt/World.xml\n",
      "http://feeds.bbci.co.uk/news/rss.xml?edition=us\n",
      "http://rss.cnn.com/rss/cnn_world.rss\n",
      "http://rss.cnn.com/rss/cnn_us.rss\n",
      "https://feeds.washingtonpost.com/rss/world?itid=lk_inline_manual_36\n",
      "https://feeds.washingtonpost.com/rss/national?itid=lk_inline_manual_32\n",
      "https://feeds.a.dj.com/rss/RSSWorldNews.xml\n",
      "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\n",
      "https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en\n"
     ]
    }
   ],
   "source": [
    "for url in rss_urls:\n",
    "    xml = get(url)\n",
    "    print(url)\n",
    "    \n",
    "    parser = Parser(xml=xml.content)\n",
    "    feed = parser.parse()\n",
    "    \n",
    "    for item in feed.feed:\n",
    "        prompts.append({\"prompt\": item.title, \"completion\": item.description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5205c0b5-0afc-4eaf-aff9-58848236399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"rss-trainingdata.jsonl\", \"w\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    f.write(json.dumps(prompt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34301b7c-8061-4988-906f-d462f213c298",
   "metadata": {},
   "source": [
    "This time we'll use a tool that OpenAI provides to clean the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f32bccda-935a-4803-b6bc-09f3b645bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 307 prompt-completion pairs\n",
      "- `completion` column/key should not contain empty strings. These are rows: [160, 162, 164, 165, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194]\n",
      "- There are 9 duplicated prompt-completion sets. These are rows: [43, 45, 49, 177, 178, 179, 210, 211, 212]\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Remove 21 rows with empty completions\n",
      "- [Recommended] Remove 9 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
      "- [Recommended] Add a suffix ending `\\n` to all completions [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `rss-trainingdata_prepared.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"rss-trainingdata_prepared.jsonl\"\n",
      "\n",
      "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 15.16 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f rss-trainingdata.jsonl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3d8cd-16c9-4598-973f-a4f5bc0b0d13",
   "metadata": {},
   "source": [
    "Let's go ahead and train a newly fine tuned model on this much larger set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbd9b689-e26c-47bf-85bf-814375fb345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = openai.File.create(file=open(\"rss-trainingdata_prepared.jsonl\"), purpose='fine-tune')\n",
    "rss_fine_tune = openai.FineTune.create(training_file=file['id'], model=\"davinci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "527503ce-dc35-4b37-8c8b-b52e7a15b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-23 16:09:28] Created fine-tune: ft-zn6ehx8QooiPqquVScmCT8gd\n",
      "[2023-02-23 19:06:21] Fine-tune costs $1.84\n",
      "[2023-02-23 19:06:22] Fine-tune enqueued\n",
      "[2023-02-23 20:30:19] Fine-tune is in the queue. Queue number: 31\n",
      "[2023-02-23 20:30:51] Fine-tune is in the queue. Queue number: 30\n",
      "[2023-02-23 20:34:11] Fine-tune is in the queue. Queue number: 29\n",
      "[2023-02-23 20:34:52] Fine-tune is in the queue. Queue number: 28\n",
      "[2023-02-23 20:35:22] Fine-tune is in the queue. Queue number: 27\n",
      "[2023-02-23 20:36:49] Fine-tune is in the queue. Queue number: 26\n",
      "[2023-02-23 20:38:21] Fine-tune is in the queue. Queue number: 25\n",
      "[2023-02-23 20:40:15] Fine-tune is in the queue. Queue number: 23\n",
      "[2023-02-23 20:40:59] Fine-tune is in the queue. Queue number: 22\n",
      "[2023-02-23 20:42:31] Fine-tune is in the queue. Queue number: 21\n",
      "[2023-02-23 20:42:46] Fine-tune is in the queue. Queue number: 20\n",
      "[2023-02-23 20:44:41] Fine-tune is in the queue. Queue number: 19\n",
      "[2023-02-23 20:44:43] Fine-tune is in the queue. Queue number: 18\n",
      "[2023-02-23 20:45:04] Fine-tune is in the queue. Queue number: 17\n",
      "[2023-02-23 20:45:58] Fine-tune is in the queue. Queue number: 16\n",
      "[2023-02-23 20:46:18] Fine-tune is in the queue. Queue number: 15\n",
      "[2023-02-23 20:47:22] Fine-tune is in the queue. Queue number: 14\n",
      "[2023-02-23 20:47:52] Fine-tune is in the queue. Queue number: 12\n",
      "[2023-02-23 20:47:53] Fine-tune is in the queue. Queue number: 12\n",
      "[2023-02-23 20:47:55] Fine-tune is in the queue. Queue number: 11\n",
      "[2023-02-23 20:51:08] Fine-tune is in the queue. Queue number: 10\n",
      "[2023-02-23 20:51:49] Fine-tune is in the queue. Queue number: 9\n",
      "[2023-02-23 20:51:50] Fine-tune is in the queue. Queue number: 8\n",
      "[2023-02-23 20:52:29] Fine-tune is in the queue. Queue number: 7\n",
      "[2023-02-23 20:52:31] Fine-tune is in the queue. Queue number: 6\n",
      "[2023-02-23 20:52:33] Fine-tune is in the queue. Queue number: 5\n",
      "[2023-02-23 20:54:17] Fine-tune is in the queue. Queue number: 4\n",
      "[2023-02-23 20:54:19] Fine-tune is in the queue. Queue number: 3\n",
      "[2023-02-23 20:54:21] Fine-tune is in the queue. Queue number: 2\n",
      "[2023-02-23 20:54:23] Fine-tune is in the queue. Queue number: 1\n",
      "[2023-02-23 20:54:24] Fine-tune is in the queue. Queue number: 0\n",
      "[2023-02-23 20:54:27] Fine-tune started\n",
      "[2023-02-23 20:58:21] Completed epoch 1/4\n",
      "[2023-02-23 20:59:45] Completed epoch 2/4\n",
      "[2023-02-23 21:01:10] Completed epoch 3/4\n",
      "[2023-02-23 21:02:34] Completed epoch 4/4\n",
      "[2023-02-23 21:03:06] Uploaded model: davinci:ft-personal-2023-02-24-04-03-06\n",
      "[2023-02-23 21:03:08] Uploaded result file: file-rdHxyejMXbdJ9eO5qSB9Lu1K\n",
      "[2023-02-23 21:03:08] Fine-tune succeeded\n",
      "\n",
      "Job complete! Status: succeeded ðŸŽ‰\n",
      "Try out your fine-tuned model:\n",
      "\n",
      "openai api completions.create -m davinci:ft-personal-2023-02-24-04-03-06 -p <YOUR_PROMPT>\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i {rss_fine_tune['id']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2d7c3-e66b-4a7a-9c9e-cf5fea98d03e",
   "metadata": {},
   "source": [
    "Once that's complete, let's compare before (non-finetuned) and after (finetuned) on a question about the last day's news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e6b6986-04ec-4fa9-8585-92d2b672382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (non-finetuned) result: \n",
      "\n",
      "Additional Information:\n",
      "\n",
      "\n",
      "\n",
      "Sound Transitâ€™s emergency closure of\n",
      "After (finetuned) result: \n",
      "\n",
      "Backgrounder\n",
      "\n",
      "In the early hours of February 10, 2019\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where did the train carrying hazardous materials derail?\"\n",
    "result = openai.Completion.create(\n",
    "    model=\"davinci\",\n",
    "    prompt=prompt + '\\n\\n###\\n\\n'\n",
    ")\n",
    "print(\"Before (non-finetuned) result: \" + result['choices'][0]['text'])\n",
    "result = openai.Completion.create(\n",
    "    model=\"davinci:ft-personal-2023-02-24-04-03-06\",\n",
    "    prompt=prompt + '\\n\\n###\\n\\n'\n",
    ")\n",
    "print(\"After (finetuned) result: \" + result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d27f3d-7de8-4a92-b170-55a034d25f22",
   "metadata": {},
   "source": [
    "Still the results are gibberish. It turns out the issue here is that when we fine tune on davinci, OpenAI is using a much older version of davinci that doesn't include the instruction following features that text-davinci-003 (or ChatGPT) include. It turns out that fine tuning is much less suited to solving instruction based problems and is much more suited towards solving problems like classification and autocompletion. To make this work the way we want it to we'll need to take a new approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e3cd3-8cf5-4eb0-92bd-832ff5798315",
   "metadata": {},
   "source": [
    "## Getting Customized Results Without Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1869532-f200-4b83-95df-6f2b029e7a95",
   "metadata": {},
   "source": [
    "Let's do an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12ed24c3-66ba-4024-8d5a-51d4187b2464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The train carrying hazardous materials derailed in East Palestine, Ohio, United States.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Given that The 2023 Ohio train derailment (also called the East Palestine train derailment) occurred on February 3, 2023, at 8:55 p.m. EST (UTCâˆ’5), when a Norfolk Southern freight train carrying hazardous materials derailed in East Palestine, Ohio, United States. Where did the train carrying hazardous materials derail?\"\n",
    "result = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt + '\\n\\n###\\n\\n'\n",
    ")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656136ba-f0b2-460f-b8b7-45801abc06da",
   "metadata": {},
   "source": [
    "This may seem counterintuitive but we presented some new information, and asked a question about the new information all without finetuning a model. If we can search for content that may provide the answer to the question being asked, prepopulate that content within the prompt, then we can use GPT-3's instructional features to work with thie new information. Fortunately, there are some great tools for coming up with creative solutions like this like langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0af1f17-1404-4922-a2ee-86ca3ea776ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /home/carter/.local/lib/python3.10/site-packages (0.0.76)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /usr/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/lib/python3.10/site-packages (from langchain) (1.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/carter/.local/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /home/carter/.local/lib/python3.10/site-packages (from langchain) (1.4.46)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/carter/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/carter/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/carter/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.12)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/carter/.local/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646d6e3-fc66-4e06-a52c-cb3bd578f83a",
   "metadata": {},
   "source": [
    "Now we'll download the same RSS feeds as before but instead we'll prefill our prompt with this data before we ask the question about current events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f311cb3e-d168-4281-b061-cb6acfc3a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "for url in rss_urls:\n",
    "    xml = get(url)\n",
    "    \n",
    "    parser = Parser(xml=xml.content)\n",
    "    feed = parser.parse()\n",
    "    \n",
    "    for item in feed.feed:\n",
    "        documents.append(Document(\n",
    "            page_content=item.title + '. ' + item.description\n",
    "        ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee8ae28d-1e27-4789-9c77-da530f6f12df",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 16655 tokens (16399 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      4\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_qa_chain(OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py:155\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py:152\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    147\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    148\u001b[0m     inputs,\n\u001b[1;32m    149\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:49\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     48\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m---> 49\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:85\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py:104\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py:155\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py:152\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    147\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    148\u001b[0m     inputs,\n\u001b[1;32m    149\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py:88\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_list: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mgenerations:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py:74\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `stop` is present in any inputs, should be present in all.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[0;32m---> 74\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py:79\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py:76\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m     73\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/openai.py:158\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    156\u001b[0m _keys \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _prompts \u001b[38;5;129;01min\u001b[39;00m sub_prompts:\n\u001b[0;32m--> 158\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    160\u001b[0m     _keys_to_use \u001b[38;5;241m=\u001b[39m _keys\u001b[38;5;241m.\u001b[39mintersection(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:227\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    217\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    218\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:680\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    678\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    681\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 16655 tokens (16399 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = \"Where did the train carrying hazardous materials derail?\"\n",
    "\n",
    "chain = load_qa_chain(OpenAI(temperature=0))\n",
    "chain({\"input_documents\":documents, \"question\":prompt}, return_only_outputs=True)[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05ce61-d31b-4d15-9922-09791e265960",
   "metadata": {},
   "source": [
    "We see here that GPT doesn't support this prompts this large, after all we dumped the entire contents of the last day's news. We'll need to take a more intelligent approach to populating this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259aa7d7-5837-4ac1-8f4a-c6b5eed75f00",
   "metadata": {},
   "source": [
    "## Using Text Embeddings and Vector Similarity Searches to Prepopulate Our Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b0967-fef8-414e-a7cc-c06e0375e294",
   "metadata": {},
   "source": [
    "Imagine if we searched for the content of our prompt within our recent news data and only populated the content that was appropriate. A typical full text search index wouldn't be appropriate here because it's very unlikely the exact words will appear in our content, especially since our content is full of statements and our prompt is a question. Instead we'll use some cutting edge technology that can determine text that's similar to other text. OpenAI recently released a Text Embeddings API which allows one to convert words into a vector that can be compared to other vectors. The vector equivalent of the statement \"people work\" would be similar to the vector equivalent of \"humans do jobs\" even though the words are completely different. If we only populate our prompt with language that's similar to our prompt we're likely to have the answer in the prompt. FAISS is a Vector Store we can use to compare text embeddings and langchain supports it as a way to build prompts for OpenAI. We'll build a search index and then only populate our prompt with information that's similar to our prompt.\n",
    "\n",
    "One note, I did have to add a payment method to my OpenAI account to overcome rate limits from generating the text embeddings from the RSS feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c0bda2f-fd24-4b03-a5ea-4053a6b8e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2af4afa2-edb5-4e6e-bbc0-5b9adb6afbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "search_index = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15bf3a1d-d83a-4948-ab71-f10653bad8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' East Palestine, Ohio.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Where did the train carrying hazardous materials derail?\"\n",
    "\n",
    "chain = load_qa_chain(OpenAI(temperature=0))\n",
    "chain({\"input_documents\":search_index.similarity_search(prompt, k=4), \"question\":prompt}, return_only_outputs=True)[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c30c5b-3aa5-46a3-8182-47fb7cbdcd4d",
   "metadata": {},
   "source": [
    "It works! By pairing the similar text search with GPT-3, we're able to now give answers about news in the RSS feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283aec93-0892-45f0-a1f0-234ea9aa9452",
   "metadata": {},
   "source": [
    "## Parting Thoughts, Shortcomings, Cavaeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6847f7-885a-4f2b-8823-87c0a278acf0",
   "metadata": {},
   "source": [
    "While this is a powerful utilization of some cutting edge Natural Language Processing, there are still a few potential shortcomings of this approach. One issue is authority. FAISS only cares about how similar text is for populating prompts. If I wanted to ask the model \"What color is the dog?\" and within my text database I see \"The dog is black\" and \"The dog is white,\" the vector index has no way of knowing which to present to the model.\n",
    "\n",
    "Another issue is that if there are many pieces of text that are similar to the prompt it will result in a very large prompt which will increase processing time and cost. \n",
    "\n",
    "All the same, we're at the beginning of a very interesting time where we seem to be reaching a critical velocity with generative AI. Tools like Langchain which have the power to accelerate already powerful tools are the sign of even more intersting times to come. With GPT-4 likely to be released any time, this year should prove to be another year of growth for AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcda3f2-42dc-40d0-9fc7-28efddefb188",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20beb408-85e2-4d6e-994c-b65ef2cd6c67",
   "metadata": {},
   "source": [
    "* https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html\n",
    "* https://dagster.io/blog/chatgpt-langchain\n",
    "* https://github.com/openai/openai-cookbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
